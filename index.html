<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_tolwdjxdxkdy-1>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-1,lower-latin) ". "}.lst-kix_tolwdjxdxkdy-8>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-8}.lst-kix_tolwdjxdxkdy-2>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-2}.lst-kix_tolwdjxdxkdy-3>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-3,decimal) ". "}ol.lst-kix_z4oqg19i6q2e-4.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-4 0}.lst-kix_tolwdjxdxkdy-2>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-2,lower-roman) ". "}.lst-kix_z4oqg19i6q2e-0>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-0}.lst-kix_tolwdjxdxkdy-5>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-5,lower-roman) ". "}ol.lst-kix_z4oqg19i6q2e-0{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-1{list-style-type:none}.lst-kix_z4oqg19i6q2e-0>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-0,decimal) ". "}ol.lst-kix_z4oqg19i6q2e-2{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-4.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-4 0}ol.lst-kix_z4oqg19i6q2e-1.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-1 0}ol.lst-kix_tolwdjxdxkdy-0.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-0 0}.lst-kix_tolwdjxdxkdy-4>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-4,lower-latin) ". "}ol.lst-kix_tolwdjxdxkdy-0{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-2{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-1{list-style-type:none}.lst-kix_z4oqg19i6q2e-1>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-1}ol.lst-kix_tolwdjxdxkdy-4{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-3{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-6{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-5{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-7.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-7 0}ol.lst-kix_z4oqg19i6q2e-7.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-7 0}.lst-kix_tolwdjxdxkdy-0>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-0,decimal) ". "}.lst-kix_tolwdjxdxkdy-3>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-3}ol.lst-kix_tolwdjxdxkdy-8.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-8 0}ol.lst-kix_tolwdjxdxkdy-1.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-1 0}ol.lst-kix_tolwdjxdxkdy-8{list-style-type:none}ol.lst-kix_tolwdjxdxkdy-7{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-5.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-5 0}ol.lst-kix_z4oqg19i6q2e-7{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-8{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-8.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-8 0}ol.lst-kix_z4oqg19i6q2e-3{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-4{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-5{list-style-type:none}ol.lst-kix_z4oqg19i6q2e-6{list-style-type:none}.lst-kix_z4oqg19i6q2e-6>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-6}.lst-kix_z4oqg19i6q2e-3>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-3}.lst-kix_tolwdjxdxkdy-5>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-5}ol.lst-kix_z4oqg19i6q2e-6.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-6 0}ol.lst-kix_tolwdjxdxkdy-5.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-5 0}.lst-kix_tolwdjxdxkdy-0>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-0}.lst-kix_tolwdjxdxkdy-6>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-6}.lst-kix_z4oqg19i6q2e-8>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-8}ol.lst-kix_z4oqg19i6q2e-2.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-2 0}.lst-kix_z4oqg19i6q2e-4>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-4}ol.lst-kix_tolwdjxdxkdy-2.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-2 0}.lst-kix_tolwdjxdxkdy-1>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-1}ol.lst-kix_tolwdjxdxkdy-3.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-3 0}.lst-kix_tolwdjxdxkdy-4>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-4}.lst-kix_tolwdjxdxkdy-7>li{counter-increment:lst-ctn-kix_tolwdjxdxkdy-7}ol.lst-kix_z4oqg19i6q2e-0.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-0 0}.lst-kix_z4oqg19i6q2e-7>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-7}.lst-kix_z4oqg19i6q2e-5>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-5}.lst-kix_z4oqg19i6q2e-6>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-6,decimal) ". "}.lst-kix_z4oqg19i6q2e-8>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-8,lower-roman) ". "}.lst-kix_z4oqg19i6q2e-2>li{counter-increment:lst-ctn-kix_z4oqg19i6q2e-2}.lst-kix_z4oqg19i6q2e-7>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-7,lower-latin) ". "}.lst-kix_z4oqg19i6q2e-2>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-2,lower-roman) ". "}.lst-kix_z4oqg19i6q2e-4>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-4,lower-latin) ". "}.lst-kix_tolwdjxdxkdy-7>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-7,lower-latin) ". "}.lst-kix_tolwdjxdxkdy-6>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-6,decimal) ". "}.lst-kix_z4oqg19i6q2e-1>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-1,lower-latin) ". "}.lst-kix_z4oqg19i6q2e-5>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-5,lower-roman) ". "}ol.lst-kix_tolwdjxdxkdy-6.start{counter-reset:lst-ctn-kix_tolwdjxdxkdy-6 0}.lst-kix_tolwdjxdxkdy-8>li:before{content:"" counter(lst-ctn-kix_tolwdjxdxkdy-8,lower-roman) ". "}.lst-kix_z4oqg19i6q2e-3>li:before{content:"" counter(lst-ctn-kix_z4oqg19i6q2e-3,decimal) ". "}ol.lst-kix_z4oqg19i6q2e-3.start{counter-reset:lst-ctn-kix_z4oqg19i6q2e-3 0}ol{margin:0;padding:0}table td,table th{padding:0}.c8{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c18{padding-top:0pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c0{padding-top:0pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c13{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c14{background-color:#ffe599;font-weight:400;font-size:14pt;font-family:"Times New Roman"}.c21{color:#000000;vertical-align:baseline;font-style:normal}.c7{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 57.6pt 72pt}.c9{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c15{font-size:14pt;font-family:"Times New Roman";font-weight:700}.c5{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c20{font-weight:700;font-size:18pt;font-family:"Times New Roman"}.c16{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c17{font-weight:400;font-size:11pt;font-family:"Arial"}.c19{font-weight:400;font-size:10pt;font-family:"Times New Roman"}.c2{margin-left:36pt;padding-left:0pt}.c11{width:33%;height:1px}.c10{padding:0;margin:0}.c6{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c7"><div><p class="c3"><span class="c13 c17"></span></p><p class="c3"><span class="c13 c17"></span></p></div><p class="c18"><span class="c13 c20">Question and Answering System</span></p><p class="c18"><span class="c5">By Vincent Lin &amp; Lester Lee</span></p><p class="c0"><span class="c5">Our Question-Answer system reads in a list of questions and generates up to 10 answers for each question by pulling relevant information from the corresponding documents. It will output these answers to the file </span><span class="c9">predictions.txt</span><span class="c4">. To run our program, use the command</span></p><p class="c0"><span class="c13 c9">python qasystem.py [start question #] [end question #]</span></p><p class="c0"><span class="c4">If no parameters are specified, it will try to answer every question from QID0 to QID300. If the QID does not appear in the question list, then it will simply skip that question.</span></p><p class="c0"><span class="c12">Question Processing</span></p><p class="c0"><span class="c5">First, we tokenize the question using NLTK&rsquo;s </span><span class="c5 c6">sent_tokenize </span><span class="c5">and </span><span class="c5 c6">word_tokenize</span><span class="c5">. We then filter all the stop words from the question using NLTK&rsquo;s </span><span class="c5 c6">stopwords</span><span class="c4">&nbsp;corpus to produce a tuple of two lists: the first list is every word that isn&rsquo;t a stopword, and the second list is every word that is a stopword. We keep track of the question&rsquo;s stopwords because that will help us decide what type of question it is later on. We also lowercase all the stopwords, such that it&rsquo;ll help us match the questions in the word. </span></p><p class="c0"><span class="c5">We do not do any further preprocessing for the questions. We remove the stopwords because </span><span class="c5">those</span><span class="c5">&nbsp;words are irrelevant to the query. In general, every single tokenizer and Named Entity Tagger have their own preprocessing processes. For example, when we passed in text without the stopwords and lowered cased into NLTK </span><span class="c5 c6">NER</span><span class="c5">&nbsp;and Stanford </span><span class="c5 c6">NER</span><span class="c5">, it called &ldquo;Bolivia, VB&rdquo;. We believe that this is due to the fact that they both use a syntactic parser for sentence processing. </span></p><p class="c0"><span class="c12">Passage Retrieval</span></p><p class="c0"><span class="c5">We use regular expressions to parse out all the text between &lt;TEXT&gt;*&lt;/TEXT&gt; tags to retrieve the text from each of the top 50 documents. We then use NLTK&rsquo;s </span><span class="c5 c6">sent_tokenize</span><span class="c5">&nbsp;and </span><span class="c5 c6">word_tokenize</span><span class="c5">&nbsp;to tokenize each document. Each document has a </span><span class="c9">score</span><span class="c5">&nbsp;(a float value); we use this to rank how relevant each document is.</span><span class="c4">&nbsp;</span></p><p class="c0"><span class="c4">For each sentences, we use frequency feature vectors to represent the question and the document, where each position in the vector represents the number of times a specific word shows up in the text. Since we are using the non-stopwords from the question as the features, the question is represented as the vector &lt;1, &hellip; , 1&gt;. &nbsp;We think this is the most reasonable implementation for the passage retrieval for our purpose since each of our passages is a sentence and the amount of time the keywords that appears in the passage is the most important one. For our top passages, we generally get 4-5 hits; this we hope eliminates weaker passages with weaker scores but more frequency.</span></p><p class="c0"><span class="c5">We do not use cosine similarity because it only takes into account the angle between two vectors, and not their magnitudes. Rather, we use </span><span class="c9">dot-product similarity</span><span class="c5">&nbsp;to calculate how similar the passage is to the question. This means that the dot-product similarity between the question and the document is simply the sum of all the frequencies of the question non-stopwords that show up in the document. However, since each document has a different </span><span class="c9">score</span><span class="c4">, and the higher score indicates that the document is more relevant, we multiply this sum by the score to obtain the document&rsquo;s final similarity score. We may able to experiment with different weights, such as multiplying by &frac12;*scores or 2*scores. Due to time consideration, we weren&rsquo;t able to do so.</span></p><p class="c0"><span class="c5">Once we calculate the similarity for every document, we take the top 5 scores. We tried taking the top 1 and 3 scores, but 5 gave us the best performance. Note, the top 5 scores means that there could be more than 5 documents, we process. We split the documents into sentences, and for each tokenized sentence, we use the </span><span class="c5 c6">Stanford Named Entity Recognizer</span><span class="c5">&nbsp;tagger to tag each named entity and NLTK&rsquo;s </span><span class="c5 c6">pos_tag</span><span class="c5">&nbsp;to get the POS tags for each token. We will use the Stanford NER and NLTK&rsquo;s pos_tag for answer processing. </span></p><p class="c0"><span class="c15">Answer Processing</span></p><p class="c0"><span class="c5">We start with an empty list labeled </span><span class="c9">answers</span><span class="c5">. Given a list of POS tagged tokens as well as named entities, we look at the question tuple generated during our Question Processing section to figure out what type of question it is. We do this by checking which word appears in the list of stopwords that were filtered out of the question. Before going through the heuristic list, we parsed out the </span><span class="c9">numeric entities</span><span class="c4">, which is classified by our NLTK pos tagger by &ldquo;CD&rdquo; followed usually by &ldquo;NNS&rdquo;. This is because the Stanford NER did not contain Named Entities including size, zip code, and other numeric entities, which is key to the answer of several questions.</span></p><p class="c0"><span class="c5 c16 c21">Heuristic Rules:</span></p><ol class="c10 lst-kix_z4oqg19i6q2e-0 start" start="1"><li class="c0 c2"><span class="c5">If the question is a </span><span class="c5 c6">where</span><span class="c5">&nbsp;question, we append the LOCATION and ORGANIZATION named entities to </span><span class="c9">answers</span><span class="c4">. </span></li><li class="c0 c2"><span class="c5">If the question is a </span><span class="c5 c6">who</span><span class="c5">&nbsp;question, we append the PERSON and ORGANIZATION named entities to </span><span class="c9">answers</span><span class="c4">. </span></li><li class="c0 c2"><span class="c5">If the question is a </span><span class="c5 c6">when</span><span class="c5">&nbsp;question, or contains the words </span><span class="c5 c6">year</span><span class="c5">, </span><span class="c5 c6">date</span><span class="c5">, or </span><span class="c5 c6">time</span><span class="c5">, we append the DATE and TIME named entities to </span><span class="c9">answers</span><span class="c4">. </span></li><li class="c0 c2"><span class="c5">If the question is a </span><span class="c5 c6">how</span><span class="c5">&nbsp;question, we append </span><span class="c9">numeric entity</span><span class="c5">, the PERCENT, MONEY, and TIME named entities to </span><span class="c9">answers</span><span class="c4">. </span></li><li class="c0 c2"><span class="c5">If the question is a </span><span class="c5 c6">name</span><span class="c5">&nbsp;question, we append the ORGANIZATION, LOCATION, and PERSON named entities to </span><span class="c9">answers</span><span class="c4">. </span></li><li class="c0 c2"><span class="c5">If the question does not contain any of the aforementioned question words, we append the 2 ORGANIZATION, 2 LOCATION, 2 PERSON, and named entities to </span><span class="c9">answers</span><span class="c4">&nbsp;because those are the most common answer types. </span></li></ol><p class="c0"><span class="c5">After going through the named entities, we also parse out each noun phrase from the list of tokens using POS tags. We then append every noun phrase to </span><span class="c9">answers</span><span class="c5">. We do this because the answer to most questions involve a noun phrase.We then append every </span><span class="c9">numeric entity </span><span class="c5">to </span><span class="c9">answers</span><span class="c4">&nbsp;since that&rsquo;s the only thing left in our list.</span></p><p class="c0"><span class="c5">Once </span><span class="c9">answers</span><span class="c5">&nbsp;has been populated with the relevant named entities and noun phrases, we sort it by frequency, since the same named entity or noun phrase may appear multiple times. Phrases with higher frequencies are given more weight, since they are likely to be relevant to the answer. We then return the top 10 elements in </span><span class="c9">answers</span><span class="c4">.</span></p><p class="c0"><span class="c4">We also implemented a feature that takes into account the distances between the keywords and the noun phrases and noun entities when it comes to answer formation. However, it turns out that this has barely improved the answer formations and in some cases make it worse and increase the runtime of the program substantially. Thus, we decided to leave it out of the final program. We thought that this is due to the fact since we are only using sentences, the noun entity should be fairly close to keywords in order to get selected. </span></p><p class="c0"><span class="c12">Competition improvements:</span></p><p class="c0"><span class="c4">For the competition, we decided to increase the size of the window of the top scores to 7 since as the top scores increases. Furthermore, we went through the test questions in an attempt to formulate better heuristic rules. We added two rules:</span></p><ol class="c10 lst-kix_tolwdjxdxkdy-0 start" start="1"><li class="c0 c2"><span class="c4">If &ldquo;zip code&rdquo; or &ldquo;population&rdquo; or &ldquo;salary&rdquo; or &ldquo;states&rdquo; appears in the question, then look for number entity.</span></li><li class="c0 c2"><span class="c4">If &ldquo;city&rdquo; or &ldquo;area&rdquo; appears in the question, treat it as the same a &ldquo;where&rdquo; question.</span></li></ol><p class="c0"><span class="c4">Since it takes about an hour to run this code, we&rsquo;ve included our prediction.txt. </span></p><p class="c0"><span class="c12">Evaluation and Analysis of Result:</span></p><p class="c0"><span class="c8">Passage Retrieval:</span></p><p class="c0"><span class="c4">Top 5 scores mmr: 0.202774</span></p><p class="c0"><span class="c4">Top 3 scores mmr: 0.192143</span></p><p class="c0"><span class="c4">Top 1 score mmr: 0.142281</span></p><p class="c0"><span class="c4">The passage Retrieval changes according to the number of top scores, we saw very little improvement from Top 3 scores to Top 5 scores. We think that Top 3 is enough to get all of the relevant passages so not many options are added to the answers list in Top 5. However, the dropoff between top 1 and 3 is too large to be accepted as trade offs for runtime.</span></p><p class="c0"><span class="c4">Our passage retrieval system is not necessarily the best as it only takes into account the number of hits each sentences are able to make. For certain question types such as the acronym question types, it is extremely beneficial to use heuristic rules such as find all the Named Entities begins with say, ACS. Furthermore, for some questions, the sentence the term is introduced might not be where the question is in. Thus, finding a better way to chuck the answers might be better for us. However, when we did experiment with 2 sentences segments, the results that it returns were worse, so we decided to keep the same sentence.</span></p><p class="c0"><span class="c9">Note</span><span class="c4">: these scores is meant to test passage retrieval, we used a more primitive version of our heuristic system without the Competition Improvements and without the numeric entity rule.</span></p><p class="c0"><span class="c8">Answer Formation:</span></p><p class="c0"><span class="c5 c16">Best possible system</span><span class="c4">: MRR: 0.208651 </span></p><p class="c0"><span class="c4">This is made with Top 5 scoring documents with Competition Improvements and Numeric Entity Improvements. Since we didn&rsquo;t have enough time, we were only able to run this program once (it takes about 45 minutes per run).</span></p><p class="c0"><span class="c4">Out of the questions we scored correct, we only gotten correct 6 noun phrase answers, with the rest 26 being the Named Entities. Obviously, most of the Jeopardy questions requires an Named Entity so this is not surprising. It appears that we&rsquo;ve performed extremely well in cases of Where, Who, and When questions. This makes sense since they require simple Named Entities which we parse easily through our document retrieval. We were also able to score some points by implementing the numeric rules which allowed us to answer some questions with numbers. Overall, I think the heuristic rules we&rsquo;ve implemented performed extremely well. Outside of these questions, the generic &ldquo;What&rdquo; questions, which requires further heuristic rule in order to perform well in. </span></p><p class="c0"><span class="c5">Questions that this system does extremely poorly (basically 0% accuracy) in are acronyms, chemical formula questions, synonym questions, definitional questions, questions that requires a verb phrase answer and questions that requires answers longer than multiple noun phrases separated by prepositions. For acronym, chemical formula and synonym questions, we need additional preprocessing and additional heuristic in passage retrieval in order to find the specific noun phrases in our answer formation documents. For definitional questions such as &ldquo;What is X?&rdquo;, questions that requires a verb phrase answer and questions that requires answers longer than multiple noun phrases separated by prepositions, the additional heuristic rules require substantially more work and may impact other question&rsquo;s accuracies.</span></p><p class="c0"><span class="c12">Future Improvements and An 8 Hour Journey into Pylucene:</span></p><p class="c0"><span class="c8">Passage Retrieval:</span></p><p class="c0"><span class="c4">After achieving the MRR of 0.208651, we&rsquo;ve decided that the remaining time could be better spent by implementing the pylucene for better passage retrieval. Our passage retrieval leaves much to be desired. When we measured the passage retrieval services, we found that the correct rate of passage retrieval is relatively low. Most notably, for one of the questions &ldquo;what is a Stratocaster?&rdquo;, our passage retrieval system yielded no passages. We would have liked to implement the feature of query expansion by stemming, better weighting mechanism, synonym query expansion with Pylucene.</span></p><p class="c0"><span class="c9">Query Expansion</span><span class="c5">: we want to expand the query search to all terms that are stemmed and lower cased from the question original terms.</span><sup class="c5"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup></p><p class="c0"><span class="c9">Synonym Query Expansion:</span><span class="c4">&nbsp;For Synonym Query Expansion, we would have taken the key query terms and identify key synonyms that are commonly associated with these terms and search for both the original query terms and the synonym terms. Synonymous terms would be given lesser weight than the original query terms. These terms can also be taken into account later in the answer formations. For example, &ldquo;when asked what baseball team does the Pittsburg has?&rdquo; We can look for synonyms &lsquo;organization&rsquo; for &lsquo;team&rsquo;, which presumably should improve our accuracy in passage retrieval.</span></p><p class="c0"><span class="c9">Distance from keywords: </span><span class="c4">For certain noun phrases, we want to take into the average distance between the words and the documents in our retrieval services. &nbsp;</span></p><p class="c0"><span class="c9">Better weighting mechanism: </span><span class="c4">Our current weighting mechanism takes into account only the frequency of relevant terms and the score of the document from which those terms were taken. In some cases, it overweighs the score of the document and prevents the most relevant documents from being retrieved. We want to implement a variety of other features, such as the average distances between noun entities. The Lucene weighting mechanism would improve the performance of our system and allow us to play around with different weighting mechanisms.</span></p><p class="c0"><span class="c9">Chunking</span><span class="c5">:</span><span class="c9">&nbsp;</span><span class="c5">We currently chunk passages by the sentence, but we could vary this window. For example, we can do 2 or 3 sentences, which may make the distance weighting more valuable.</span></p><p class="c0"><span class="c9">Further Heuristics in Passage Retrieval</span><span class="c4">: We only use heuristics in answer formation, so for &nbsp;some questions, the relevant documents are not retrieved. The most relevant example is the acronym questions, such as &ldquo;What does the NASA stand for?&rdquo; or &ldquo;What is the chemical formula for X?&rdquo;. Currently, our success rate for these types of questions is 0%. Since we don&rsquo;t look for passages with the explicit words for NASA, one way to improve on it is to use heuristic rules such as &ldquo;if the question is in the form of &nbsp;&lsquo;what does X stand for?&rdquo;, then search the passage for all 4 words noun phrases and named entities whose words begin with N, A, S, A in that sequence. Other form of heuristic rules would also be helpful in determining the most relevant passages to each respective questions. </span></p><p class="c0"><span class="c13 c9">Adventure in PyLucene:</span></p><p class="c0"><span class="c4">Once we decided to implement the PyLucene, we had to install both JCC and Apache Ant, which was a nightmare to install in Mac. Furthermore, there is very little documentation available for PyLucene. We eventually had to look through PyLucene sample code, which we only found useful after several experimentations. PyLucene requires us to split one top doc into separate document text files for each respective passages; this generation alone took 40 minutes. We would then have to use the search function to go through each respective documents and make query using the SearchFile function. At this point, we decided it was not worth the time to finish implementing this system as the features we wanted PyLucene to perform required too much time investment.</span></p><p class="c0"><span class="c4">For references, we&rsquo;ve included in the directory the IndexFile.py and SearchFile.py. The IndexFile.py takes in a directory of topdocs and index them so then you can use SearchFile.py to perform specific searches on them.</span></p><p class="c0"><span class="c8">Answer Retrieval:</span></p><p class="c0"><span class="c4">For Answer Retrieval, we may consider two types of improvements, query reformulation and additional heuristic rules.</span></p><p class="c0"><span class="c9">Query/Question Reformulation</span><span class="c4">: The Query Reformulation reformulates the query, which is in the form of certain question types as one specific type. For example, we can reformulate various questions asking for the same thing very differently. &ldquo;What is the color of the Williams College?&rdquo; and &ldquo;Name the color of Williams College&rdquo; are both asking for the same thing. Similarly, there are many other different question types that we can reformulate into a standardized question. </span></p><p class="c0"><span class="c9">Additional Heuristic Rules</span><span class="c5">: More linguistic analysis on certain formats of the questions will allow us to come up with more human rules and increase the MRR of our system.</span></p><div><p class="c3"><span class="c13 c17"></span></p></div><hr class="c11"><div><p class="c1"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c13 c19">&nbsp;Note a group of University of Washington graduate students found that query expansion added more noise to the Passage Retrieval system than helping it. This was done on TREC 2004 and 2005 data.</span></p><p class="c1"><span class="c13 c19">http://faculty.washington.edu/levow/courses/ling573_SPR2011/slides/D3_group_6.pdfhttp://www2.lingfil.uu.se/SLTC2014/abstracts/sltc2014_submission_3.pdf<br></span></p></div></body></html>